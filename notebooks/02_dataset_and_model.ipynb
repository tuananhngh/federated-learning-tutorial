{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 2: Understanding the Dataset and Model\n",
    "\n",
    "In this notebook, we'll explore:\n",
    "1. The FashionMNIST dataset\n",
    "2. Data partitioning for federated learning\n",
    "3. The CNN model architecture\n",
    "4. Training and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring FashionMNIST Dataset\n",
    "\n",
    "FashionMNIST is a dataset of fashion product images:\n",
    "- **70,000 grayscale images** (60,000 training + 10,000 test)\n",
    "- **28x28 pixels** per image\n",
    "- **10 classes**: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, Ankle boot\n",
    "\n",
    "It's a drop-in replacement for MNIST, but more challenging!\n",
    "\n",
    "Let's load and visualize some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1d4f8dd61774e7fb76be00881cda6bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d093ee761146b5860a6ea05ea9f962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "fashion_mnist/train-00000-of-00001.parqu(…):   0%|          | 0.00/30.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \u001b[2m2025-10-19T17:44:00.547743Z\u001b[0m \u001b[31mERROR\u001b[0m  \u001b[31mPython exception updating progress:, error: PyErr { type: <class 'LookupError'>, value: LookupError(<ContextVar name='shell_parent' at 0xffffb1183ec0>), traceback: Some(<traceback object at 0xffff28b038c0>) }, \u001b[1;31mcaller\u001b[0m\u001b[31m: \"src/progress_update.rs:313\"\u001b[0m\n",
      "    \u001b[2;3mat\u001b[0m /home/runner/work/xet-core/xet-core/error_printer/src/lib.rs:28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load FashionMNIST dataset\n",
    "dataset = load_dataset(\"zalando-datasets/fashion_mnist\", split=\"train\")\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)} images\")\n",
    "print(f\"Dataset features: {dataset.features}\")\n",
    "print(f\"\\nFirst sample keys: {dataset[0].keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class names for FashionMNIST\n",
    "class_names = [\n",
    "    'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "    'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "]\n",
    "\n",
    "# Visualize some images\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6))\n",
    "fig.suptitle('Sample Images from FashionMNIST', fontsize=16)\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    img = dataset[idx]['image']\n",
    "    label = dataset[idx]['label']\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'{class_names[label]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Partitioning for Federated Learning\n",
    "\n",
    "In federated learning, data is distributed across multiple clients. We'll explore two partitioning strategies:\n",
    "\n",
    "### IID (Independent and Identically Distributed)\n",
    "- Each client gets a random subset of data\n",
    "- Class distribution is similar across clients\n",
    "- **Easier scenario** for federated learning\n",
    "\n",
    "### Non-IID (Non-Independent and Identically Distributed)\n",
    "- Each client may have different class distributions\n",
    "- **More realistic** but challenging scenario\n",
    "- Example: One client has mostly t-shirts, another has mostly trousers\n",
    "\n",
    "For this tutorial, we'll use **IID partitioning** to keep things simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create federated dataset with IID partitioning\n",
    "num_partitions = 5  # Simulate 5 clients\n",
    "partitioner = IidPartitioner(num_partitions=num_partitions)\n",
    "\n",
    "fds = FederatedDataset(\n",
    "    dataset=\"zalando-datasets/fashion_mnist\",\n",
    "    partitioners={\"train\": partitioner},\n",
    ")\n",
    "\n",
    "print(f\"Created federated dataset with {num_partitions} partitions\")\n",
    "print(f\"Each partition will have approximately {60000 // num_partitions} training images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's examine the class distribution for each partition\n",
    "def get_class_distribution(partition):\n",
    "    \"\"\"Count the number of samples per class in a partition.\"\"\"\n",
    "    labels = [sample['label'] for sample in partition]\n",
    "    return np.bincount(labels, minlength=10)\n",
    "\n",
    "# Plot class distribution for each client\n",
    "fig, axes = plt.subplots(1, num_partitions, figsize=(20, 4))\n",
    "fig.suptitle('Class Distribution Across Clients (IID Partitioning)', fontsize=16)\n",
    "\n",
    "for client_id in range(num_partitions):\n",
    "    partition = fds.load_partition(client_id)\n",
    "    distribution = get_class_distribution(partition)\n",
    "    \n",
    "    axes[client_id].bar(range(10), distribution, color='steelblue')\n",
    "    axes[client_id].set_title(f'Client {client_id}')\n",
    "    axes[client_id].set_xlabel('Class')\n",
    "    axes[client_id].set_ylabel('Count')\n",
    "    axes[client_id].set_xticks(range(10))\n",
    "    axes[client_id].set_xticklabels(range(10))\n",
    "    axes[client_id].set_ylim(0, 1500)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice how the distribution is roughly uniform across all clients.\")\n",
    "print(\"This is characteristic of IID partitioning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The CNN Model Architecture\n",
    "\n",
    "We'll use a simple Convolutional Neural Network (CNN) for image classification:\n",
    "\n",
    "```\n",
    "Input (28x28x1) - Grayscale image\n",
    "    ↓\n",
    "Conv2D (6 filters, 5x5) → ReLU → MaxPool (2x2)\n",
    "    ↓\n",
    "Conv2D (16 filters, 5x5) → ReLU → MaxPool (2x2)\n",
    "    ↓\n",
    "Flatten\n",
    "    ↓\n",
    "FC (120) → ReLU\n",
    "    ↓\n",
    "FC (84) → ReLU\n",
    "    ↓\n",
    "FC (10) → Output\n",
    "```\n",
    "\n",
    "This is a classic architecture adapted for FashionMNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes: int = 10) -> None:\n",
    "        super(Net, self).__init__()\n",
    "        # Convolutional layers (for 28x28 grayscale images)\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(in_features=16 * 4 * 4, out_features=120)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=num_classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # First conv block: 28x28x1 -> 24x24x6 -> 12x12x6\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Second conv block: 12x12x6 -> 8x8x16 -> 4x4x16\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten: 4x4x16 = 256\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create model and count parameters\n",
    "model = Net()\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing and Loading\n",
    "\n",
    "Before training, we need to:\n",
    "1. Convert images to tensors\n",
    "2. Normalize pixel values\n",
    "3. Create data loaders for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms for grayscale images\n",
    "pytorch_transforms = Compose([\n",
    "    ToTensor(),  # Convert PIL Image to tensor (0-1 range)\n",
    "    Normalize((0.5,), (0.5,))  # Normalize to (-1, 1) range for grayscale\n",
    "])\n",
    "\n",
    "def apply_transforms(batch):\n",
    "    \"\"\"Apply transforms to a batch of images.\"\"\"\n",
    "    batch[\"image\"] = [pytorch_transforms(img) for img in batch[\"image\"]]\n",
    "    return batch\n",
    "\n",
    "# Load data for one client\n",
    "client_id = 0\n",
    "partition = fds.load_partition(client_id)\n",
    "\n",
    "# Split into train/test\n",
    "partition_train_test = partition.train_test_split(test_size=0.2, seed=42)\n",
    "partition_train_test = partition_train_test.with_transform(apply_transforms)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 32\n",
    "trainloader = DataLoader(partition_train_test[\"train\"], batch_size=batch_size, shuffle=True)\n",
    "testloader = DataLoader(partition_train_test[\"test\"], batch_size=batch_size)\n",
    "\n",
    "print(f\"Client {client_id} data:\")\n",
    "print(f\"  Training samples: {len(partition_train_test['train'])}\")\n",
    "print(f\"  Test samples: {len(partition_train_test['test'])}\")\n",
    "print(f\"  Training batches: {len(trainloader)}\")\n",
    "print(f\"  Test batches: {len(testloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a batch after preprocessing\n",
    "batch = next(iter(trainloader))\n",
    "images = batch['image'][:8]\n",
    "labels = batch['label'][:8]\n",
    "\n",
    "# Denormalize for visualization\n",
    "def denormalize(img):\n",
    "    img = img * 0.5 + 0.5  # Reverse normalization\n",
    "    return torch.clamp(img, 0, 1)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "fig.suptitle('Preprocessed Images (from a batch)', fontsize=16)\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    img = denormalize(images[idx]).squeeze().numpy()\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    ax.set_title(f'{class_names[labels[idx]]}')\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training and Testing Functions\n",
    "\n",
    "Let's implement the training and testing logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, trainloader, epochs, lr, device):\n",
    "    \"\"\"Train the model on the training set.\"\"\"\n",
    "    net.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
    "    net.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for batch in trainloader:\n",
    "            images = batch[\"image\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        running_loss += epoch_loss / len(trainloader)\n",
    "        print(f\"  Epoch {epoch+1}/{epochs}: Loss = {epoch_loss/len(trainloader):.4f}\")\n",
    "    \n",
    "    avg_trainloss = running_loss / epochs\n",
    "    return avg_trainloss\n",
    "\n",
    "def test(net, testloader, device):\n",
    "    \"\"\"Validate the model on the test set.\"\"\"\n",
    "    net.to(device)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    net.eval()\n",
    "    \n",
    "    correct, loss = 0, 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images = batch[\"image\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "            outputs = net(images)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            correct += (torch.max(outputs.data, 1)[1] == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / len(testloader.dataset)\n",
    "    loss = loss / len(testloader)\n",
    "    return loss, accuracy\n",
    "\n",
    "print(\"Training and testing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Quick Training Test\n",
    "\n",
    "Let's do a quick test to ensure everything works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test training for 2 epochs\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\\n\")\n",
    "\n",
    "model = Net()\n",
    "print(\"Training for 2 epochs...\")\n",
    "train_loss = train(model, trainloader, epochs=2, lr=0.01, device=device)\n",
    "\n",
    "print(f\"\\nTesting model...\")\n",
    "test_loss, test_acc = test(model, testloader, device)\n",
    "\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  Average training loss: {train_loss:.4f}\")\n",
    "print(f\"  Test loss: {test_loss:.4f}\")\n",
    "print(f\"  Test accuracy: {test_acc*100:.2f}%\")\n",
    "print(\"\\nEverything works! The model is learning.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Model Predictions\n",
    "\n",
    "Let's see what the model predicts on some test images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of test images\n",
    "model.eval()\n",
    "batch = next(iter(testloader))\n",
    "images = batch['image'][:8].to(device)\n",
    "labels = batch['label'][:8]\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(images)\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "# Visualize\n",
    "images = images.cpu()\n",
    "predicted = predicted.cpu()\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(12, 6))\n",
    "fig.suptitle('Model Predictions', fontsize=16)\n",
    "\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    img = denormalize(images[idx]).squeeze().numpy()\n",
    "    ax.imshow(img, cmap='gray')\n",
    "    \n",
    "    true_label = class_names[labels[idx]]\n",
    "    pred_label = class_names[predicted[idx]]\n",
    "    color = 'green' if labels[idx] == predicted[idx] else 'red'\n",
    "    \n",
    "    ax.set_title(f'True: {true_label}\\nPred: {pred_label}', color=color, fontsize=9)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Green titles = correct predictions\")\n",
    "print(\"Red titles = incorrect predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we:\n",
    "1. ✅ Explored the FashionMNIST dataset\n",
    "2. ✅ Understood IID data partitioning\n",
    "3. ✅ Built a CNN model architecture\n",
    "4. ✅ Implemented training and testing functions\n",
    "5. ✅ Verified everything works with a quick test\n",
    "6. ✅ Visualized model predictions\n",
    "\n",
    "**Next Steps**: In Notebook 3, we'll learn how to wrap this code into a Flower client for federated learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises for Students\n",
    "\n",
    "**Exercise 1**: Modify the code to use Non-IID partitioning. (Hint: Look at `flwr_datasets.partitioner.DirichletPartitioner`)\n",
    "\n",
    "**Exercise 2**: Count the number of parameters in each layer of the CNN. Which layer has the most parameters?\n",
    "\n",
    "**Exercise 3**: What happens if you train for more epochs (e.g., 5)? Does accuracy improve?\n",
    "\n",
    "**Exercise 4**: Modify the model to add one more convolutional layer. How does this affect the number of parameters?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
