{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Running the Full Federated Learning Experiment\n",
    "\n",
    "In this final notebook, we'll:\n",
    "1. Set up configuration for the FL experiment\n",
    "2. Run federated learning with Flower CLI\n",
    "3. Analyze the results\n",
    "4. Compare with centralized training\n",
    "5. Explore next steps and advanced topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Experiment\n",
    "\n",
    "Before running federated learning, we need to configure:\n",
    "- Number of clients\n",
    "- Number of rounds\n",
    "- Learning rate\n",
    "- Local epochs per round\n",
    "- Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for our experiment\n",
    "config = {\n",
    "    \"num_clients\": 5,\n",
    "    \"num_server_rounds\": 10,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"local_epochs\": 3,\n",
    "    \"batch_size\": 32,\n",
    "    \"fraction_evaluate\": 1.0,  # Use all clients for evaluation\n",
    "}\n",
    "\n",
    "print(\"Federated Learning Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in config.items():\n",
    "    print(f\"{key:.<30} {value}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding the Flower Configuration File\n",
    "\n",
    "Flower uses YAML configuration files to set up experiments. Here's what a typical config looks like:\n",
    "\n",
    "```yaml\n",
    "# pyproject.toml\n",
    "[tool.flwr.app]\n",
    "publisher = \"flwrlabs\"\n",
    "\n",
    "[tool.flwr.app.components]\n",
    "serverapp = \"fltutorial.server:app\"\n",
    "clientapp = \"fltutorial.client:app\"\n",
    "\n",
    "[tool.flwr.app.config]\n",
    "num-server-rounds = 10\n",
    "learning-rate = 0.01\n",
    "local-epochs = 3\n",
    "batch-size = 32\n",
    "fraction-evaluate = 1.0\n",
    "\n",
    "[tool.flwr.federations]\n",
    "default = \"local-simulation\"\n",
    "\n",
    "[tool.flwr.federations.local-simulation]\n",
    "options.num-supernodes = 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running Federated Learning\n",
    "\n",
    "To run the experiment, we use the Flower CLI:\n",
    "\n",
    "```bash\n",
    "# Run in simulation mode (all clients on same machine)\n",
    "flwr run\n",
    "```\n",
    "\n",
    "This command:\n",
    "1. Reads configuration from `pyproject.toml`\n",
    "2. Starts the server\n",
    "3. Spawns the specified number of client processes\n",
    "4. Runs the federated learning for the configured number of rounds\n",
    "5. Saves the final model\n",
    "\n",
    "**Note**: We can't run this directly in a notebook because it requires multiple processes. See the next section for how to run it from the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Alternative: Simulation with Flower Simulation\n",
    "\n",
    "For educational purposes, let's see how to set up a simple simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is a simplified example for understanding\n",
    "# Real federated learning should use 'flwr run' command\n",
    "\n",
    "import torch\n",
    "from fltutorial.task import Net, load_data, train as train_fn, test as test_fn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Setting up simulation...\")\n",
    "print(\"This demonstrates the FL process but isn't actual federated learning\")\n",
    "print(\"For real FL, use: flwr run\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing Results\n",
    "\n",
    "After running federated learning, we can analyze:\n",
    "- Training curves (loss and accuracy over rounds)\n",
    "- Final model performance\n",
    "- Per-client performance\n",
    "- Communication costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Visualizing training progress\n",
    "# These would be real metrics from your FL run\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Simulated results for demonstration\n",
    "rounds = np.arange(1, 11)\n",
    "train_loss = [2.3, 1.5, 1.0, 0.8, 0.6, 0.5, 0.45, 0.42, 0.40, 0.38]\n",
    "test_accuracy = [15, 42, 58, 68, 75, 78, 80, 81, 82, 83]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curve\n",
    "ax1.plot(rounds, train_loss, 'o-', linewidth=2, markersize=8, color='#e74c3c')\n",
    "ax1.set_xlabel('Federated Round', fontsize=12)\n",
    "ax1.set_ylabel('Average Training Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Over Federated Rounds', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(rounds)\n",
    "\n",
    "# Accuracy curve\n",
    "ax2.plot(rounds, test_accuracy, 's-', linewidth=2, markersize=8, color='#27ae60')\n",
    "ax2.set_xlabel('Federated Round', fontsize=12)\n",
    "ax2.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Test Accuracy Over Federated Rounds', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks(rounds)\n",
    "ax2.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Test Accuracy: {test_accuracy[-1]:.2f}%\")\n",
    "print(f\"Final Training Loss: {train_loss[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing Federated vs Centralized Learning\n",
    "\n",
    "Let's compare federated learning with traditional centralized learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Aspect': [\n",
    "        'Data Privacy',\n",
    "        'Communication Cost',\n",
    "        'Training Speed',\n",
    "        'Final Accuracy',\n",
    "        'Scalability',\n",
    "        'Regulatory Compliance'\n",
    "    ],\n",
    "    'Centralized': [\n",
    "        'Low (all data in one place)',\n",
    "        'Low (no model exchange)',\n",
    "        'Fast',\n",
    "        'High (typically 85-90%)',\n",
    "        'Limited by server capacity',\n",
    "        'Difficult (data aggregation)'\n",
    "    ],\n",
    "    'Federated': [\n",
    "        'High (data stays local)',\n",
    "        'Higher (model exchange)',\n",
    "        'Slower (multiple rounds)',\n",
    "        'Good (typically 80-85%)',\n",
    "        'High (distributed compute)',\n",
    "        'Easier (GDPR compliant)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Centralized vs Federated Learning Comparison\")\n",
    "print(\"=\" * 100)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracy comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "methods = ['Centralized\\nLearning', 'Federated\\nLearning\\n(IID)', 'Federated\\nLearning\\n(Non-IID)']\n",
    "accuracies = [87, 83, 78]\n",
    "colors = ['#3498db', '#27ae60', '#e67e22']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(methods, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc}%',\n",
    "            ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "ax.set_title('FashionMNIST Classification: Centralized vs Federated', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Centralized learning achieves highest accuracy (all data together)\")\n",
    "print(\"- Federated learning (IID) is close, with ~4% accuracy drop\")\n",
    "print(\"- Non-IID data makes FL more challenging (~9% drop)\")\n",
    "print(\"- Privacy benefits of FL outweigh small accuracy loss in many cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Understanding Communication Costs\n",
    "\n",
    "One important aspect of federated learning is communication efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate communication costs\n",
    "from fltutorial.task import Net\n",
    "import torch\n",
    "\n",
    "model = Net()\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "bytes_per_param = 4  # float32\n",
    "model_size_mb = (num_params * bytes_per_param) / (1024 * 1024)\n",
    "\n",
    "num_clients = 5\n",
    "num_rounds = 10\n",
    "\n",
    "# Each round: server sends to all clients, clients send back\n",
    "total_communication_mb = num_rounds * num_clients * 2 * model_size_mb\n",
    "\n",
    "print(\"Communication Cost Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "print(f\"Model size: {model_size_mb:.2f} MB\")\n",
    "print(f\"\\nPer round:\")\n",
    "print(f\"  Server → Clients: {num_clients * model_size_mb:.2f} MB\")\n",
    "print(f\"  Clients → Server: {num_clients * model_size_mb:.2f} MB\")\n",
    "print(f\"  Total per round: {num_clients * 2 * model_size_mb:.2f} MB\")\n",
    "print(f\"\\nTotal for {num_rounds} rounds:\")\n",
    "print(f\"  {total_communication_mb:.2f} MB\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Comparison with centralized\n",
    "# In centralized, you'd send all data once\n",
    "dataset_size_mb = 60000 * 28 * 28 / (1024 * 1024)  # FashionMNIST training set\n",
    "print(f\"\\nFor comparison:\")\n",
    "print(f\"Sending all FashionMNIST training data: {dataset_size_mb:.2f} MB\")\n",
    "print(f\"FL communication overhead: {(total_communication_mb / dataset_size_mb):.2f}x\")\n",
    "print(\"\\nBut remember: In FL, raw data NEVER leaves the client!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Loading and Testing the Final Model\n",
    "\n",
    "After FL training completes, we can load and test the final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code would work after running 'flwr run'\n",
    "# It loads the saved model and tests it\n",
    "\n",
    "import torch\n",
    "from fltutorial.task import Net, load_centralized_dataset, test as test_fn\n",
    "\n",
    "# Load the final model (if it exists)\n",
    "model_path = \"final_model.pt\"\n",
    "\n",
    "try:\n",
    "    # Load saved model\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Test on centralized test set\n",
    "    test_dataloader = load_centralized_dataset()\n",
    "    test_loss, test_acc = test_fn(model, test_dataloader, device)\n",
    "    \n",
    "    print(\"Final Model Performance\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(\"=\" * 50)\n",
    "    \nexcept FileNotFoundError:\n",
    "    print(\"No saved model found.\")\n",
    "    print(\"Run 'flwr run' first to train the model.\")\n",
    "    print(\"\\nFor now, this notebook demonstrates the concepts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Running the Real Experiment\n",
    "\n",
    "To run the actual federated learning experiment:\n",
    "\n",
    "### Step 1: Open a terminal in the project directory\n",
    "\n",
    "### Step 2: Run the Flower command\n",
    "```bash\n",
    "flwr run\n",
    "```\n",
    "\n",
    "### Step 3: Watch the output\n",
    "You'll see:\n",
    "- Server starting\n",
    "- Clients connecting\n",
    "- Training progress for each round\n",
    "- Global model evaluation after each round\n",
    "- Final model being saved\n",
    "\n",
    "### Step 4: Analyze results\n",
    "Come back to this notebook and run the analysis cells above!\n",
    "\n",
    "### Alternative: Docker Setup\n",
    "```bash\n",
    "# Build and run with docker-compose\n",
    "docker-compose up\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced Topics and Next Steps\n",
    "\n",
    "Now that you understand the basics, here are advanced topics to explore:\n",
    "\n",
    "### 1. **Non-IID Data**\n",
    "   - Use `DirichletPartitioner` instead of `IidPartitioner`\n",
    "   - Observe how heterogeneous data affects convergence\n",
    "   - Experiment with different alpha values\n",
    "\n",
    "### 2. **Advanced Aggregation Strategies**\n",
    "   - **FedProx**: Adds a proximal term to handle heterogeneity\n",
    "   - **FedAdam**: Uses adaptive learning rates\n",
    "   - **FedBN**: Doesn't aggregate batch normalization layers\n",
    "\n",
    "### 3. **Differential Privacy**\n",
    "   - Add noise to model updates\n",
    "   - Implement DP-SGD in the client\n",
    "   - Balance privacy and accuracy\n",
    "\n",
    "### 4. **Communication Efficiency**\n",
    "   - **Gradient compression**: Reduce communication by compressing updates\n",
    "   - **Partial aggregation**: Only update some layers\n",
    "   - **Quantization**: Use lower precision for communication\n",
    "\n",
    "### 5. **Cross-Device vs Cross-Silo**\n",
    "   - **Cross-device**: Many mobile devices (phones, IoT)\n",
    "   - **Cross-silo**: Few organizations (hospitals, banks)\n",
    "   - Different challenges and solutions\n",
    "\n",
    "### 6. **Secure Aggregation**\n",
    "   - Prevent server from seeing individual updates\n",
    "   - Homomorphic encryption\n",
    "   - Secure multi-party computation\n",
    "\n",
    "### 7. **Personalization**\n",
    "   - Fine-tune global model for each client\n",
    "   - Meta-learning approaches\n",
    "   - Per-client model customization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Resources for Further Learning\n",
    "\n",
    "### Documentation\n",
    "- **Flower Docs**: https://flower.ai/docs/\n",
    "- **Flower Examples**: https://github.com/adap/flower/tree/main/examples\n",
    "\n",
    "### Papers\n",
    "- **Original FedAvg**: McMahan et al. (2017) \"Communication-Efficient Learning of Deep Networks from Decentralized Data\"\n",
    "- **FedProx**: Li et al. (2020) \"Federated Optimization in Heterogeneous Networks\"\n",
    "- **DP-FL**: Geyer et al. (2017) \"Differentially Private Federated Learning\"\n",
    "\n",
    "### Courses\n",
    "- Stanford CS329S: Machine Learning Systems Design\n",
    "- Andrew Ng's Privacy-Preserving ML course\n",
    "\n",
    "### Communities\n",
    "- Flower Slack community\n",
    "- r/MachineLearning on Reddit\n",
    "- Federated Learning subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've completed the Federated Learning tutorial! 🎉\n",
    "\n",
    "### What you learned:\n",
    "1. ✅ Federated learning concepts and motivation\n",
    "2. ✅ FashionMNIST dataset and data partitioning\n",
    "3. ✅ Building CNN models for image classification\n",
    "4. ✅ Implementing Flower clients\n",
    "5. ✅ Implementing Flower servers and FedAvg\n",
    "6. ✅ Running complete FL experiments\n",
    "7. ✅ Analyzing results and comparing with centralized learning\n",
    "8. ✅ Understanding communication costs\n",
    "\n",
    "### Your Journey:\n",
    "```\n",
    "Introduction → Dataset → Model → Client → Server → Experiment\n",
    "                                                         ↓\n",
    "                                              🎯 You are here!\n",
    "```\n",
    "\n",
    "### What's Next?\n",
    "- Experiment with different configurations\n",
    "- Try non-IID data partitioning\n",
    "- Implement your own aggregation strategy\n",
    "- Apply FL to your own datasets\n",
    "- Contribute to the Flower community!\n",
    "\n",
    "**Happy Federated Learning!** 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Exercises\n",
    "\n",
    "**Exercise 1**: Run the FL experiment with different numbers of clients (2, 5, 10). How does it affect convergence?\n",
    "\n",
    "**Exercise 2**: Change the local epochs parameter. What happens with more/fewer local epochs?\n",
    "\n",
    "**Exercise 3**: Implement a function to visualize the class distribution across clients for non-IID partitioning.\n",
    "\n",
    "**Exercise 4**: Research and propose a solution for the \"stragglers problem\" (when some clients are much slower than others).\n",
    "\n",
    "**Challenge Project**: Implement federated learning for a different dataset (e.g., MNIST, CIFAR-10) or a different task (e.g., sentiment analysis, time series forecasting)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
