{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Running the Full Federated Learning Experiment\n",
    "\n",
    "In this final notebook, we'll:\n",
    "1. Set up configuration for the FL experiment\n",
    "2. Run federated learning with Flower CLI\n",
    "3. Analyze the results\n",
    "4. Compare with centralized training\n",
    "5. Explore next steps and advanced topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting Up the Experiment\n",
    "\n",
    "Before running federated learning, we need to configure:\n",
    "- Number of clients\n",
    "- Number of rounds\n",
    "- Learning rate\n",
    "- Local epochs per round\n",
    "- Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for our experiment\n",
    "config = {\n",
    "    \"num_clients\": 5,\n",
    "    \"num_server_rounds\": 10,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"local_epochs\": 3,\n",
    "    \"batch_size\": 32,\n",
    "    \"fraction_evaluate\": 1.0,  # Use all clients for evaluation\n",
    "}\n",
    "\n",
    "print(\"Federated Learning Configuration:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in config.items():\n",
    "    print(f\"{key:.<30} {value}\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding the Flower Configuration File\n",
    "\n",
    "Flower uses YAML configuration files to set up experiments. Here's what a typical config looks like:\n",
    "\n",
    "```yaml\n",
    "# pyproject.toml\n",
    "[tool.flwr.app]\n",
    "publisher = \"flwrlabs\"\n",
    "\n",
    "[tool.flwr.app.components]\n",
    "serverapp = \"fltutorial.server:app\"\n",
    "clientapp = \"fltutorial.client:app\"\n",
    "\n",
    "[tool.flwr.app.config]\n",
    "num-server-rounds = 10\n",
    "learning-rate = 0.01\n",
    "local-epochs = 3\n",
    "batch-size = 32\n",
    "fraction-evaluate = 1.0\n",
    "\n",
    "[tool.flwr.federations]\n",
    "default = \"local-simulation\"\n",
    "\n",
    "[tool.flwr.federations.local-simulation]\n",
    "options.num-supernodes = 5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Running Federated Learning\n",
    "\n",
    "To run the experiment, we use the Flower CLI:\n",
    "\n",
    "```bash\n",
    "# Run in simulation mode (all clients on same machine)\n",
    "flwr run\n",
    "```\n",
    "\n",
    "This command:\n",
    "1. Reads configuration from `pyproject.toml`\n",
    "2. Starts the server\n",
    "3. Spawns the specified number of client processes\n",
    "4. Runs the federated learning for the configured number of rounds\n",
    "5. Saves the final model\n",
    "\n",
    "**Note**: We can't run this directly in a notebook because it requires multiple processes. See the next section for how to run it from the terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Alternative: Simulation with Flower Simulation\n",
    "\n",
    "For educational purposes, let's see how to set up a simple simulation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is a simplified example for understanding\n",
    "# Real federated learning should use 'flwr run' command\n",
    "\n",
    "import torch\n",
    "from fltutorial.task import Net, load_data, train as train_fn, test as test_fn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Setting up simulation...\")\n",
    "print(\"This demonstrates the FL process but isn't actual federated learning\")\n",
    "print(\"For real FL, use: flwr run\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyzing Results\n",
    "\n",
    "After running federated learning, we can analyze:\n",
    "- Training curves (loss and accuracy over rounds)\n",
    "- Final model performance\n",
    "- Per-client performance\n",
    "- Communication costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Visualizing training progress\n",
    "# These would be real metrics from your FL run\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Simulated results for demonstration\n",
    "rounds = np.arange(1, 11)\n",
    "train_loss = [2.3, 1.5, 1.0, 0.8, 0.6, 0.5, 0.45, 0.42, 0.40, 0.38]\n",
    "test_accuracy = [15, 42, 58, 68, 75, 78, 80, 81, 82, 83]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curve\n",
    "ax1.plot(rounds, train_loss, 'o-', linewidth=2, markersize=8, color='#e74c3c')\n",
    "ax1.set_xlabel('Federated Round', fontsize=12)\n",
    "ax1.set_ylabel('Average Training Loss', fontsize=12)\n",
    "ax1.set_title('Training Loss Over Federated Rounds', fontsize=14, fontweight='bold')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_xticks(rounds)\n",
    "\n",
    "# Accuracy curve\n",
    "ax2.plot(rounds, test_accuracy, 's-', linewidth=2, markersize=8, color='#27ae60')\n",
    "ax2.set_xlabel('Federated Round', fontsize=12)\n",
    "ax2.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "ax2.set_title('Test Accuracy Over Federated Rounds', fontsize=14, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xticks(rounds)\n",
    "ax2.set_ylim(0, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final Test Accuracy: {test_accuracy[-1]:.2f}%\")\n",
    "print(f\"Final Training Loss: {train_loss[-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparing Federated vs Centralized Learning\n",
    "\n",
    "Let's compare federated learning with traditional centralized learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Comparison table\n",
    "comparison = pd.DataFrame({\n",
    "    'Aspect': [\n",
    "        'Data Privacy',\n",
    "        'Communication Cost',\n",
    "        'Training Speed',\n",
    "        'Final Accuracy',\n",
    "        'Scalability',\n",
    "        'Regulatory Compliance'\n",
    "    ],\n",
    "    'Centralized': [\n",
    "        'Low (all data in one place)',\n",
    "        'Low (no model exchange)',\n",
    "        'Fast',\n",
    "        'High (typically 85-90%)',\n",
    "        'Limited by server capacity',\n",
    "        'Difficult (data aggregation)'\n",
    "    ],\n",
    "    'Federated': [\n",
    "        'High (data stays local)',\n",
    "        'Higher (model exchange)',\n",
    "        'Slower (multiple rounds)',\n",
    "        'Good (typically 80-85%)',\n",
    "        'High (distributed compute)',\n",
    "        'Easier (GDPR compliant)'\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Centralized vs Federated Learning Comparison\")\n",
    "print(\"=\" * 100)\n",
    "print(comparison.to_string(index=False))\n",
    "print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize accuracy comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "methods = ['Centralized\\nLearning', 'Federated\\nLearning\\n(IID)', 'Federated\\nLearning\\n(Non-IID)']\n",
    "accuracies = [87, 83, 78]\n",
    "colors = ['#3498db', '#27ae60', '#e67e22']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "bars = ax.bar(methods, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=2)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc}%',\n",
    "            ha='center', va='bottom', fontsize=14, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Test Accuracy (%)', fontsize=12)\n",
    "ax.set_title('FashionMNIST Classification: Centralized vs Federated', fontsize=14, fontweight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"- Centralized learning achieves highest accuracy (all data together)\")\n",
    "print(\"- Federated learning (IID) is close, with ~4% accuracy drop\")\n",
    "print(\"- Non-IID data makes FL more challenging (~9% drop)\")\n",
    "print(\"- Privacy benefits of FL outweigh small accuracy loss in many cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Understanding Communication Costs\n",
    "\n",
    "One important aspect of federated learning is communication efficiency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate communication costs\n",
    "from fltutorial.task import Net\n",
    "import torch\n",
    "\n",
    "model = Net()\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "bytes_per_param = 4  # float32\n",
    "model_size_mb = (num_params * bytes_per_param) / (1024 * 1024)\n",
    "\n",
    "num_clients = 5\n",
    "num_rounds = 10\n",
    "\n",
    "# Each round: server sends to all clients, clients send back\n",
    "total_communication_mb = num_rounds * num_clients * 2 * model_size_mb\n",
    "\n",
    "print(\"Communication Cost Analysis\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Model parameters: {num_params:,}\")\n",
    "print(f\"Model size: {model_size_mb:.2f} MB\")\n",
    "print(f\"\\nPer round:\")\n",
    "print(f\"  Server â†’ Clients: {num_clients * model_size_mb:.2f} MB\")\n",
    "print(f\"  Clients â†’ Server: {num_clients * model_size_mb:.2f} MB\")\n",
    "print(f\"  Total per round: {num_clients * 2 * model_size_mb:.2f} MB\")\n",
    "print(f\"\\nTotal for {num_rounds} rounds:\")\n",
    "print(f\"  {total_communication_mb:.2f} MB\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Comparison with centralized\n",
    "# In centralized, you'd send all data once\n",
    "dataset_size_mb = 60000 * 28 * 28 / (1024 * 1024)  # FashionMNIST training set\n",
    "print(f\"\\nFor comparison:\")\n",
    "print(f\"Sending all FashionMNIST training data: {dataset_size_mb:.2f} MB\")\n",
    "print(f\"FL communication overhead: {(total_communication_mb / dataset_size_mb):.2f}x\")\n",
    "print(\"\\nBut remember: In FL, raw data NEVER leaves the client!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Loading and Testing the Final Model\n",
    "\n",
    "After FL training completes, we can load and test the final model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code would work after running 'flwr run'\n",
    "# It loads the saved model and tests it\n",
    "\n",
    "import torch\n",
    "from fltutorial.task import Net, load_centralized_dataset, test as test_fn\n",
    "\n",
    "# Load the final model (if it exists)\n",
    "model_path = \"final_model.pt\"\n",
    "\n",
    "try:\n",
    "    # Load saved model\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Test on centralized test set\n",
    "    test_dataloader = load_centralized_dataset()\n",
    "    test_loss, test_acc = test_fn(model, test_dataloader, device)\n",
    "    \n",
    "    print(\"Final Model Performance\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "    print(\"=\" * 50)\n",
    "    \nexcept FileNotFoundError:\n",
    "    print(\"No saved model found.\")\n",
    "    print(\"Run 'flwr run' first to train the model.\")\n",
    "    print(\"\\nFor now, this notebook demonstrates the concepts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Running the Real Experiment\n",
    "\n",
    "To run the actual federated learning experiment:\n",
    "\n",
    "### Step 1: Open a terminal in the project directory\n",
    "\n",
    "### Step 2: Run the Flower command\n",
    "```bash\n",
    "flwr run\n",
    "```\n",
    "\n",
    "### Step 3: Watch the output\n",
    "You'll see:\n",
    "- Server starting\n",
    "- Clients connecting\n",
    "- Training progress for each round\n",
    "- Global model evaluation after each round\n",
    "- Final model being saved\n",
    "\n",
    "### Step 4: Analyze results\n",
    "Come back to this notebook and run the analysis cells above!\n",
    "\n",
    "### Alternative: Docker Setup\n",
    "```bash\n",
    "# Build and run with docker-compose\n",
    "docker-compose up\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced Topics and Next Steps\n",
    "\n",
    "Now that you understand the basics, here are advanced topics to explore:\n",
    "\n",
    "### 1. **Non-IID Data**\n",
    "   - Use `DirichletPartitioner` instead of `IidPartitioner`\n",
    "   - Observe how heterogeneous data affects convergence\n",
    "   - Experiment with different alpha values\n",
    "\n",
    "### 2. **Advanced Aggregation Strategies**\n",
    "   - **FedProx**: Adds a proximal term to handle heterogeneity\n",
    "   - **FedAdam**: Uses adaptive learning rates\n",
    "   - **FedBN**: Doesn't aggregate batch normalization layers\n",
    "\n",
    "### 3. **Differential Privacy**\n",
    "   - Add noise to model updates\n",
    "   - Implement DP-SGD in the client\n",
    "   - Balance privacy and accuracy\n",
    "\n",
    "### 4. **Communication Efficiency**\n",
    "   - **Gradient compression**: Reduce communication by compressing updates\n",
    "   - **Partial aggregation**: Only update some layers\n",
    "   - **Quantization**: Use lower precision for communication\n",
    "\n",
    "### 5. **Cross-Device vs Cross-Silo**\n",
    "   - **Cross-device**: Many mobile devices (phones, IoT)\n",
    "   - **Cross-silo**: Few organizations (hospitals, banks)\n",
    "   - Different challenges and solutions\n",
    "\n",
    "### 6. **Secure Aggregation**\n",
    "   - Prevent server from seeing individual updates\n",
    "   - Homomorphic encryption\n",
    "   - Secure multi-party computation\n",
    "\n",
    "### 7. **Personalization**\n",
    "   - Fine-tune global model for each client\n",
    "   - Meta-learning approaches\n",
    "   - Per-client model customization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Resources for Further Learning\n",
    "\n",
    "### Documentation\n",
    "- **Flower Docs**: https://flower.ai/docs/\n",
    "- **Flower Examples**: https://github.com/adap/flower/tree/main/examples\n",
    "\n",
    "### Papers\n",
    "- **Original FedAvg**: McMahan et al. (2017) \"Communication-Efficient Learning of Deep Networks from Decentralized Data\"\n",
    "- **FedProx**: Li et al. (2020) \"Federated Optimization in Heterogeneous Networks\"\n",
    "- **DP-FL**: Geyer et al. (2017) \"Differentially Private Federated Learning\"\n",
    "\n",
    "### Courses\n",
    "- Stanford CS329S: Machine Learning Systems Design\n",
    "- Andrew Ng's Privacy-Preserving ML course\n",
    "\n",
    "### Communities\n",
    "- Flower Slack community\n",
    "- r/MachineLearning on Reddit\n",
    "- Federated Learning subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've completed the Federated Learning tutorial! ðŸŽ‰\n",
    "\n",
    "### What you learned:\n",
    "1. âœ… Federated learning concepts and motivation\n",
    "2. âœ… FashionMNIST dataset and data partitioning\n",
    "3. âœ… Building CNN models for image classification\n",
    "4. âœ… Implementing Flower clients\n",
    "5. âœ… Implementing Flower servers and FedAvg\n",
    "6. âœ… Running complete FL experiments\n",
    "7. âœ… Analyzing results and comparing with centralized learning\n",
    "8. âœ… Understanding communication costs\n",
    "\n",
    "### Your Journey:\n",
    "```\n",
    "Introduction â†’ Dataset â†’ Model â†’ Client â†’ Server â†’ Experiment\n",
    "                                                         â†“\n",
    "                                              ðŸŽ¯ You are here!\n",
    "```\n",
    "\n",
    "### What's Next?\n",
    "- Experiment with different configurations\n",
    "- Try non-IID data partitioning\n",
    "- Implement your own aggregation strategy\n",
    "- Apply FL to your own datasets\n",
    "- Contribute to the Flower community!\n",
    "\n",
    "**Happy Federated Learning!** ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Exercises\n",
    "\n",
    "**Exercise 1**: Run the FL experiment with different numbers of clients (2, 5, 10). How does it affect convergence?\n",
    "\n",
    "**Exercise 2**: Change the local epochs parameter. What happens with more/fewer local epochs?\n",
    "\n",
    "**Exercise 3**: Implement a function to visualize the class distribution across clients for non-IID partitioning.\n",
    "\n",
    "**Exercise 4**: Research and propose a solution for the \"stragglers problem\" (when some clients are much slower than others).\n",
    "\n",
    "**Challenge Project**: Implement federated learning for a different dataset (e.g., MNIST, CIFAR-10) or a different task (e.g., sentiment analysis, time series forecasting)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
